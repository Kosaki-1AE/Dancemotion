{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e6d8863-d2a0-49a1-b915-3ae8ab9b38ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m classifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm Vidushan and I live in Japan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('ner')\n",
    "classifier(\"Hello I'm Vidushan and I live in Japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef2c6bd1-f30b-445d-be2a-4b129ddf201d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.32.4-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: requests in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Downloading fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Requirement already satisfied: colorama in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Downloading transformers-4.52.4-py3-none-any.whl (10.5 MB)\n",
      "   ---------------------------------------- 0.0/10.5 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 4.2/10.5 MB 35.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.5/10.5 MB 32.6 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.32.4-py3-none-any.whl (512 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "Downloading fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
      "Downloading numpy-2.2.6-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 12.9/12.9 MB 67.2 MB/s eta 0:00:00\n",
      "Using cached regex-2024.11.6-cp310-cp310-win_amd64.whl (274 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-win_amd64.whl (308 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tqdm, safetensors, regex, numpy, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "\n",
      "   ---------------------------------------- 0/9 [tqdm]\n",
      "   ---- ----------------------------------- 1/9 [safetensors]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ------------- -------------------------- 3/9 [numpy]\n",
      "   ----------------- ---------------------- 4/9 [fsspec]\n",
      "   ----------------- ---------------------- 4/9 [fsspec]\n",
      "   ----------------- ---------------------- 4/9 [fsspec]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   -------------------------- ------------- 6/9 [huggingface-hub]\n",
      "   ------------------------------- -------- 7/9 [tokenizers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ----------------------------------- ---- 8/9 [transformers]\n",
      "   ---------------------------------------- 9/9 [transformers]\n",
      "\n",
      "Successfully installed filelock-3.18.0 fsspec-2025.5.1 huggingface-hub-0.32.4 numpy-2.2.6 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 tqdm-4.67.1 transformers-4.52.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1dc2078-de36-46fd-b65f-3307dbd57cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m----> 3\u001b[0m classifier \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mner\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m classifier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello I\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm Vidushan and I live in Japan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 942\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    943\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m    944\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    945\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    946\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    947\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    950\u001b[0m     )\n\u001b[0;32m    952\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    953\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32mD:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\pipelines\\base.py:243\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[38;5;124;03mSelect framework (TensorFlow or PyTorch) to use from the `model` passed. Returns a tuple (framework, model).\u001b[39;00m\n\u001b[0;32m    219\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m    `Tuple`: A tuple framework, model.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available():\n\u001b[1;32m--> 243\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of TensorFlow 2.0 or PyTorch should be installed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo install PyTorch, read the instructions at https://pytorch.org/.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m     )\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    249\u001b[0m     model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_pipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m task\n",
      "\u001b[1;31mRuntimeError\u001b[0m: At least one of TensorFlow 2.0 or PyTorch should be installed. To install TensorFlow 2.0, read the instructions at https://www.tensorflow.org/install/ To install PyTorch, read the instructions at https://pytorch.org/."
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"ner\")\n",
    "classifier(\"Hello I'm Vidushan and I live in Japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6e06dad-c70f-47dc-85fa-c8f1ef79c3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting TensorFlow\n",
      "  Downloading tensorflow-2.19.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from TensorFlow)\n",
      "  Downloading absl_py-2.3.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting astunparse>=1.6.0 (from TensorFlow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from TensorFlow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from TensorFlow)\n",
      "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from TensorFlow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from TensorFlow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from TensorFlow)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from TensorFlow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from TensorFlow)\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from TensorFlow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from TensorFlow) (57.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from TensorFlow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from TensorFlow)\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from TensorFlow) (4.13.2)\n",
      "Collecting wrapt>=1.11.0 (from TensorFlow)\n",
      "  Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from TensorFlow)\n",
      "  Downloading grpcio-1.72.1-cp310-cp310-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard~=2.19.0 (from TensorFlow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from TensorFlow)\n",
      "  Downloading keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0 (from TensorFlow)\n",
      "  Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting h5py>=3.11.0 (from TensorFlow)\n",
      "  Downloading h5py-3.13.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from TensorFlow)\n",
      "  Downloading ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl.metadata (22 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from TensorFlow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->TensorFlow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->TensorFlow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->TensorFlow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->TensorFlow) (2025.4.26)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->TensorFlow)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->TensorFlow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->TensorFlow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->TensorFlow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->TensorFlow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->TensorFlow)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->TensorFlow)\n",
      "  Downloading optree-0.16.0-cp310-cp310-win_amd64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->TensorFlow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->TensorFlow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from rich->keras>=3.5.0->TensorFlow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->TensorFlow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "   ---------------------------------------- 0.0/375.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 15.5/375.7 MB 74.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 27.5/375.7 MB 67.2 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 43.0/375.7 MB 68.4 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 60.0/375.7 MB 70.8 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 78.9/375.7 MB 74.1 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 95.7/375.7 MB 76.3 MB/s eta 0:00:04\n",
      "   ----------- --------------------------- 110.1/375.7 MB 74.8 MB/s eta 0:00:04\n",
      "   ------------ -------------------------- 123.7/375.7 MB 73.2 MB/s eta 0:00:04\n",
      "   -------------- ------------------------ 137.4/375.7 MB 72.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------ 138.7/375.7 MB 66.1 MB/s eta 0:00:04\n",
      "   --------------- ----------------------- 154.1/375.7 MB 66.6 MB/s eta 0:00:04\n",
      "   ----------------- --------------------- 170.1/375.7 MB 67.1 MB/s eta 0:00:04\n",
      "   ------------------- ------------------- 186.1/375.7 MB 68.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------ 201.6/375.7 MB 68.5 MB/s eta 0:00:03\n",
      "   ---------------------- ---------------- 218.6/375.7 MB 69.2 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 235.7/375.7 MB 70.1 MB/s eta 0:00:02\n",
      "   ------------------------- ------------- 248.8/375.7 MB 69.5 MB/s eta 0:00:02\n",
      "   --------------------------- ----------- 262.1/375.7 MB 69.0 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 275.8/375.7 MB 68.7 MB/s eta 0:00:02\n",
      "   ----------------------------- --------- 283.9/375.7 MB 67.9 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 291.5/375.7 MB 66.0 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 298.6/375.7 MB 64.2 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 307.2/375.7 MB 62.5 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 316.7/375.7 MB 61.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ----- 325.3/375.7 MB 59.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ---- 335.3/375.7 MB 58.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 346.0/375.7 MB 57.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 357.6/375.7 MB 56.2 MB/s eta 0:00:01\n",
      "   --------------------------------------  368.3/375.7 MB 55.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 55.7 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.7/375.7 MB 55.7 MB/s eta 0:00:01\n",
      "   --------------------------------------- 375.7/375.7 MB 51.9 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.72.1-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.2/4.2 MB 50.8 MB/s eta 0:00:00\n",
      "Downloading ml_dtypes-0.5.1-cp310-cp310-win_amd64.whl (209 kB)\n",
      "Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ---------------------------------------  12.8/12.9 MB 62.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 57.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading absl_py-2.3.0-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.13.0-cp310-cp310-win_amd64.whl (3.0 MB)\n",
      "   ---------------------------------------- 0.0/3.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.0/3.0 MB 57.2 MB/s eta 0:00:00\n",
      "Downloading keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 36.1 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 12.6/26.4 MB 65.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.2/26.4 MB 66.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 62.0 MB/s eta 0:00:00\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 39.5 MB/s eta 0:00:00\n",
      "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wrapt-1.17.2-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp310-cp310-win_amd64.whl (304 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, grpcio, google-pasta, gast, absl-py, tensorboard, ml-dtypes, markdown-it-py, h5py, astunparse, rich, keras, TensorFlow\n",
      "\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   - --------------------------------------  1/27 [libclang]\n",
      "   -- -------------------------------------  2/27 [flatbuffers]\n",
      "   ----- ----------------------------------  4/27 [wheel]\n",
      "   ----- ----------------------------------  4/27 [wheel]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------- --------------------------------  5/27 [werkzeug]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   ------------- --------------------------  9/27 [protobuf]\n",
      "   -------------- ------------------------- 10/27 [optree]\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "  Attempting uninstall: numpy\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "    Found existing installation: numpy 2.2.6\n",
      "   ---------------- ----------------------- 11/27 [opt-einsum]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "    Uninstalling numpy-2.2.6:\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   ----------------- ---------------------- 12/27 [numpy]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   -------------------- ------------------- 14/27 [markdown]\n",
      "   ---------------------- ----------------- 15/27 [grpcio]\n",
      "   ---------------------- ----------------- 15/27 [grpcio]\n",
      "   ---------------------- ----------------- 15/27 [grpcio]\n",
      "   ----------------------- ---------------- 16/27 [google-pasta]\n",
      "   ----------------------- ---------------- 16/27 [google-pasta]\n",
      "   -------------------------- ------------- 18/27 [absl-py]\n",
      "   -------------------------- ------------- 18/27 [absl-py]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ---------------------------- ----------- 19/27 [tensorboard]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   ------------------------------- -------- 21/27 [markdown-it-py]\n",
      "   -------------------------------- ------- 22/27 [h5py]\n",
      "   -------------------------------- ------- 22/27 [h5py]\n",
      "   -------------------------------- ------- 22/27 [h5py]\n",
      "   -------------------------------- ------- 22/27 [h5py]\n",
      "   ---------------------------------- ----- 23/27 [astunparse]\n",
      "   ----------------------------------- ---- 24/27 [rich]\n",
      "   ----------------------------------- ---- 24/27 [rich]\n",
      "   ----------------------------------- ---- 24/27 [rich]\n",
      "   ----------------------------------- ---- 24/27 [rich]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   ------------------------------------- -- 25/27 [keras]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   -------------------------------------- - 26/27 [TensorFlow]\n",
      "   ---------------------------------------- 27/27 [TensorFlow]\n",
      "\n",
      "Successfully installed TensorFlow-2.19.0 absl-py-2.3.0 astunparse-1.6.3 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.72.1 h5py-3.13.0 keras-3.10.0 libclang-18.1.1 markdown-3.8 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.1.3 opt-einsum-3.4.0 optree-0.16.0 protobuf-5.29.5 rich-14.0.0 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-io-gcs-filesystem-0.31.0 termcolor-3.1.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\otake\\gpu-service\\users\\vidzshan\\venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\otake\\gpu-service\\users\\vidzshan\\venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3e67a22-d3bd-4edf-888d-00ab5725c3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-nightly\n",
      "  Downloading tf_nightly-2.20.0.dev20250605-cp310-cp310-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (25.0)\n",
      "Requirement already satisfied: protobuf>=4.21.6 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (57.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (1.72.1)\n",
      "Collecting tb-nightly~=2.19.0.a (from tf-nightly)\n",
      "  Downloading tb_nightly-2.19.0a20250218-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras-nightly>=3.6.0.dev (from tf-nightly)\n",
      "  Downloading keras_nightly-3.10.0.dev2025060503-py3-none-any.whl.metadata (6.0 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (3.13.0)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-nightly) (0.5.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tf-nightly) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tf-nightly) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tf-nightly) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tf-nightly) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tb-nightly~=2.19.0.a->tf-nightly) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tb-nightly~=2.19.0.a->tf-nightly) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from astunparse>=1.6.0->tf-nightly) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from keras-nightly>=3.6.0.dev->tf-nightly) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tb-nightly~=2.19.0.a->tf-nightly) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from rich->keras-nightly>=3.6.0.dev->tf-nightly) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras-nightly>=3.6.0.dev->tf-nightly) (0.1.2)\n",
      "Downloading tf_nightly-2.20.0.dev20250605-cp310-cp310-win_amd64.whl (326.9 MB)\n",
      "   ---------------------------------------- 0.0/326.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/326.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/326.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.0/326.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/326.9 MB 2.6 MB/s eta 0:02:08\n",
      "   ---------------------------------------- 2.1/326.9 MB 2.6 MB/s eta 0:02:08\n",
      "   ---------------------------------------- 2.1/326.9 MB 2.6 MB/s eta 0:02:08\n",
      "   ---------------------------------------- 2.1/326.9 MB 2.6 MB/s eta 0:02:08\n",
      "   ---------------------------------------- 3.1/326.9 MB 2.0 MB/s eta 0:02:42\n",
      "   ---------------------------------------- 3.1/326.9 MB 2.0 MB/s eta 0:02:42\n",
      "   ---------------------------------------- 3.1/326.9 MB 2.0 MB/s eta 0:02:42\n",
      "    --------------------------------------- 4.2/326.9 MB 1.8 MB/s eta 0:02:59\n",
      "    --------------------------------------- 4.2/326.9 MB 1.8 MB/s eta 0:02:59\n",
      "    --------------------------------------- 4.2/326.9 MB 1.8 MB/s eta 0:02:59\n",
      "    --------------------------------------- 5.2/326.9 MB 1.7 MB/s eta 0:03:07\n",
      "    --------------------------------------- 5.2/326.9 MB 1.7 MB/s eta 0:03:07\n",
      "    --------------------------------------- 6.3/326.9 MB 1.8 MB/s eta 0:02:54\n",
      "    --------------------------------------- 7.3/326.9 MB 2.1 MB/s eta 0:02:36\n",
      "   - -------------------------------------- 8.4/326.9 MB 2.2 MB/s eta 0:02:24\n",
      "   - -------------------------------------- 8.4/326.9 MB 2.2 MB/s eta 0:02:24\n",
      "   - -------------------------------------- 8.4/326.9 MB 2.2 MB/s eta 0:02:24\n",
      "   - -------------------------------------- 8.4/326.9 MB 2.2 MB/s eta 0:02:24\n",
      "   - -------------------------------------- 9.4/326.9 MB 2.1 MB/s eta 0:02:31\n",
      "   - -------------------------------------- 9.4/326.9 MB 2.1 MB/s eta 0:02:31\n",
      "   - -------------------------------------- 11.5/326.9 MB 2.3 MB/s eta 0:02:20\n",
      "   - -------------------------------------- 11.5/326.9 MB 2.3 MB/s eta 0:02:20\n",
      "   - -------------------------------------- 12.6/326.9 MB 2.3 MB/s eta 0:02:16\n",
      "   - -------------------------------------- 13.6/326.9 MB 2.4 MB/s eta 0:02:09\n",
      "   - -------------------------------------- 14.7/326.9 MB 2.5 MB/s eta 0:02:04\n",
      "   - -------------------------------------- 15.7/326.9 MB 2.6 MB/s eta 0:01:59\n",
      "   -- ------------------------------------- 16.8/326.9 MB 2.7 MB/s eta 0:01:55\n",
      "   -- ------------------------------------- 17.8/326.9 MB 2.8 MB/s eta 0:01:51\n",
      "   -- ------------------------------------- 18.9/326.9 MB 2.9 MB/s eta 0:01:48\n",
      "   -- ------------------------------------- 19.9/326.9 MB 2.9 MB/s eta 0:01:47\n",
      "   -- ------------------------------------- 19.9/326.9 MB 2.9 MB/s eta 0:01:47\n",
      "   -- ------------------------------------- 19.9/326.9 MB 2.9 MB/s eta 0:01:47\n",
      "   -- ------------------------------------- 19.9/326.9 MB 2.9 MB/s eta 0:01:47\n",
      "   -- ------------------------------------- 21.0/326.9 MB 2.7 MB/s eta 0:01:52\n",
      "   -- ------------------------------------- 22.0/326.9 MB 2.8 MB/s eta 0:01:49\n",
      "   -- ------------------------------------- 23.1/326.9 MB 2.9 MB/s eta 0:01:47\n",
      "   -- ------------------------------------- 23.1/326.9 MB 2.9 MB/s eta 0:01:47\n",
      "   -- ------------------------------------- 23.1/326.9 MB 2.9 MB/s eta 0:01:47\n",
      "   -- ------------------------------------- 24.1/326.9 MB 2.8 MB/s eta 0:01:50\n",
      "   -- ------------------------------------- 24.1/326.9 MB 2.8 MB/s eta 0:01:50\n",
      "   -- ------------------------------------- 24.1/326.9 MB 2.8 MB/s eta 0:01:50\n",
      "   --- ------------------------------------ 25.2/326.9 MB 2.7 MB/s eta 0:01:54\n",
      "   --- ------------------------------------ 25.2/326.9 MB 2.7 MB/s eta 0:01:54\n",
      "   --- ------------------------------------ 25.2/326.9 MB 2.7 MB/s eta 0:01:54\n",
      "   --- ------------------------------------ 26.2/326.9 MB 2.6 MB/s eta 0:01:55\n",
      "   --- ------------------------------------ 26.2/326.9 MB 2.6 MB/s eta 0:01:55\n",
      "   --- ------------------------------------ 26.2/326.9 MB 2.6 MB/s eta 0:01:55\n",
      "   --- ------------------------------------ 27.3/326.9 MB 2.6 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 27.3/326.9 MB 2.6 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 27.3/326.9 MB 2.6 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 27.3/326.9 MB 2.6 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 28.3/326.9 MB 2.5 MB/s eta 0:02:01\n",
      "   --- ------------------------------------ 29.4/326.9 MB 2.5 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 29.4/326.9 MB 2.5 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 30.4/326.9 MB 2.5 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 30.4/326.9 MB 2.5 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 30.4/326.9 MB 2.5 MB/s eta 0:01:58\n",
      "   --- ------------------------------------ 31.5/326.9 MB 2.5 MB/s eta 0:02:00\n",
      "   --- ------------------------------------ 32.5/326.9 MB 2.5 MB/s eta 0:01:57\n",
      "   ---- ----------------------------------- 33.6/326.9 MB 2.6 MB/s eta 0:01:55\n",
      "   ---- ----------------------------------- 34.6/326.9 MB 2.6 MB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 34.6/326.9 MB 2.6 MB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 34.6/326.9 MB 2.6 MB/s eta 0:01:53\n",
      "   ---- ----------------------------------- 35.7/326.9 MB 2.6 MB/s eta 0:01:54\n",
      "   ---- ----------------------------------- 35.7/326.9 MB 2.6 MB/s eta 0:01:54\n",
      "   ---- ----------------------------------- 36.7/326.9 MB 2.6 MB/s eta 0:01:54\n",
      "   ---- ----------------------------------- 36.7/326.9 MB 2.6 MB/s eta 0:01:54\n",
      "   ---- ----------------------------------- 36.7/326.9 MB 2.6 MB/s eta 0:01:54\n",
      "   ---- ----------------------------------- 37.7/326.9 MB 2.5 MB/s eta 0:01:56\n",
      "   ---- ----------------------------------- 37.7/326.9 MB 2.5 MB/s eta 0:01:56\n",
      "   ---- ----------------------------------- 38.8/326.9 MB 2.5 MB/s eta 0:01:55\n",
      "   ---- ----------------------------------- 38.8/326.9 MB 2.5 MB/s eta 0:01:55\n",
      "   ---- ----------------------------------- 38.8/326.9 MB 2.5 MB/s eta 0:01:55\n",
      "   ---- ----------------------------------- 39.8/326.9 MB 2.5 MB/s eta 0:01:56\n",
      "   ----- ---------------------------------- 40.9/326.9 MB 2.5 MB/s eta 0:01:54\n",
      "   ----- ---------------------------------- 41.9/326.9 MB 2.5 MB/s eta 0:01:53\n",
      "   ----- ---------------------------------- 43.0/326.9 MB 2.6 MB/s eta 0:01:51\n",
      "   ----- ---------------------------------- 44.0/326.9 MB 2.6 MB/s eta 0:01:49\n",
      "   ----- ---------------------------------- 44.0/326.9 MB 2.6 MB/s eta 0:01:49\n",
      "   ----- ---------------------------------- 44.0/326.9 MB 2.6 MB/s eta 0:01:49\n",
      "   ----- ---------------------------------- 45.1/326.9 MB 2.6 MB/s eta 0:01:51\n",
      "   ----- ---------------------------------- 46.1/326.9 MB 2.6 MB/s eta 0:01:49\n",
      "   ----- ---------------------------------- 47.2/326.9 MB 2.6 MB/s eta 0:01:47\n",
      "   ----- ---------------------------------- 48.2/326.9 MB 2.7 MB/s eta 0:01:46\n",
      "   ------ --------------------------------- 49.3/326.9 MB 2.7 MB/s eta 0:01:44\n",
      "   ------ --------------------------------- 50.3/326.9 MB 2.7 MB/s eta 0:01:43\n",
      "   ------ --------------------------------- 51.4/326.9 MB 2.7 MB/s eta 0:01:42\n",
      "   ------ --------------------------------- 51.4/326.9 MB 2.7 MB/s eta 0:01:42\n",
      "   ------ --------------------------------- 52.4/326.9 MB 2.7 MB/s eta 0:01:41\n",
      "   ------ --------------------------------- 53.5/326.9 MB 2.8 MB/s eta 0:01:40\n",
      "   ------ --------------------------------- 53.5/326.9 MB 2.8 MB/s eta 0:01:40\n",
      "   ------ --------------------------------- 54.5/326.9 MB 2.8 MB/s eta 0:01:39\n",
      "   ------ --------------------------------- 55.6/326.9 MB 2.8 MB/s eta 0:01:38\n",
      "   ------ --------------------------------- 56.6/326.9 MB 2.8 MB/s eta 0:01:37\n",
      "   ------- -------------------------------- 57.7/326.9 MB 2.8 MB/s eta 0:01:36\n",
      "   ------- -------------------------------- 58.2/326.9 MB 2.8 MB/s eta 0:01:36\n",
      "   ------- -------------------------------- 58.7/326.9 MB 2.8 MB/s eta 0:01:35\n",
      "   ------- -------------------------------- 58.7/326.9 MB 2.8 MB/s eta 0:01:35\n",
      "   ------- -------------------------------- 58.7/326.9 MB 2.8 MB/s eta 0:01:35\n",
      "   ------- -------------------------------- 58.7/326.9 MB 2.8 MB/s eta 0:01:35\n",
      "   ------- -------------------------------- 58.7/326.9 MB 2.8 MB/s eta 0:01:35\n",
      "   ------- -------------------------------- 59.8/326.9 MB 2.7 MB/s eta 0:01:38\n",
      "   ------- -------------------------------- 59.8/326.9 MB 2.7 MB/s eta 0:01:38\n",
      "   ------- -------------------------------- 59.8/326.9 MB 2.7 MB/s eta 0:01:38\n",
      "   ------- -------------------------------- 60.8/326.9 MB 2.7 MB/s eta 0:01:39\n",
      "   ------- -------------------------------- 61.9/326.9 MB 2.7 MB/s eta 0:01:38\n",
      "   ------- -------------------------------- 62.9/326.9 MB 2.7 MB/s eta 0:01:37\n",
      "   ------- -------------------------------- 64.0/326.9 MB 2.8 MB/s eta 0:01:36\n",
      "   ------- -------------------------------- 65.0/326.9 MB 2.8 MB/s eta 0:01:35\n",
      "   -------- ------------------------------- 66.1/326.9 MB 2.8 MB/s eta 0:01:34\n",
      "   -------- ------------------------------- 67.1/326.9 MB 2.8 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 67.1/326.9 MB 2.8 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 67.1/326.9 MB 2.8 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 68.2/326.9 MB 2.8 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 68.2/326.9 MB 2.8 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 68.2/326.9 MB 2.8 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 69.2/326.9 MB 2.8 MB/s eta 0:01:34\n",
      "   -------- ------------------------------- 70.3/326.9 MB 2.8 MB/s eta 0:01:33\n",
      "   -------- ------------------------------- 71.3/326.9 MB 2.8 MB/s eta 0:01:32\n",
      "   -------- ------------------------------- 71.3/326.9 MB 2.8 MB/s eta 0:01:32\n",
      "   -------- ------------------------------- 71.3/326.9 MB 2.8 MB/s eta 0:01:32\n",
      "   -------- ------------------------------- 71.3/326.9 MB 2.8 MB/s eta 0:01:32\n",
      "   -------- ------------------------------- 71.3/326.9 MB 2.8 MB/s eta 0:01:32\n",
      "   -------- ------------------------------- 72.4/326.9 MB 2.7 MB/s eta 0:01:34\n",
      "   -------- ------------------------------- 73.4/326.9 MB 2.7 MB/s eta 0:01:33\n",
      "   --------- ------------------------------ 74.4/326.9 MB 2.8 MB/s eta 0:01:32\n",
      "   --------- ------------------------------ 75.5/326.9 MB 2.8 MB/s eta 0:01:31\n",
      "   --------- ------------------------------ 76.5/326.9 MB 2.8 MB/s eta 0:01:30\n",
      "   --------- ------------------------------ 77.6/326.9 MB 2.8 MB/s eta 0:01:29\n",
      "   --------- ------------------------------ 78.6/326.9 MB 2.8 MB/s eta 0:01:28\n",
      "   --------- ------------------------------ 79.7/326.9 MB 2.8 MB/s eta 0:01:27\n",
      "   --------- ------------------------------ 80.7/326.9 MB 2.9 MB/s eta 0:01:26\n",
      "   ---------- ----------------------------- 81.8/326.9 MB 2.9 MB/s eta 0:01:26\n",
      "   ---------- ----------------------------- 81.8/326.9 MB 2.9 MB/s eta 0:01:26\n",
      "   ---------- ----------------------------- 82.8/326.9 MB 2.9 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 82.8/326.9 MB 2.9 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 82.8/326.9 MB 2.9 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 83.9/326.9 MB 2.8 MB/s eta 0:01:26\n",
      "   ---------- ----------------------------- 84.9/326.9 MB 2.9 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 84.9/326.9 MB 2.9 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 84.9/326.9 MB 2.9 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 84.9/326.9 MB 2.9 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 86.2/326.9 MB 2.8 MB/s eta 0:01:25\n",
      "   ---------- ----------------------------- 87.0/326.9 MB 2.9 MB/s eta 0:01:24\n",
      "   ---------- ----------------------------- 88.1/326.9 MB 2.9 MB/s eta 0:01:23\n",
      "   ---------- ----------------------------- 89.1/326.9 MB 2.9 MB/s eta 0:01:22\n",
      "   ---------- ----------------------------- 89.1/326.9 MB 2.9 MB/s eta 0:01:22\n",
      "   ----------- ---------------------------- 90.2/326.9 MB 3.0 MB/s eta 0:01:20\n",
      "   ----------- ---------------------------- 91.0/326.9 MB 3.0 MB/s eta 0:01:20\n",
      "   ----------- ---------------------------- 91.2/326.9 MB 3.0 MB/s eta 0:01:20\n",
      "   ----------- ---------------------------- 93.3/326.9 MB 3.0 MB/s eta 0:01:18\n",
      "   ----------- ---------------------------- 94.4/326.9 MB 3.1 MB/s eta 0:01:17\n",
      "   ----------- ---------------------------- 94.4/326.9 MB 3.1 MB/s eta 0:01:17\n",
      "   ----------- ---------------------------- 94.4/326.9 MB 3.1 MB/s eta 0:01:17\n",
      "   ----------- ---------------------------- 94.4/326.9 MB 3.1 MB/s eta 0:01:17\n",
      "   ----------- ---------------------------- 95.4/326.9 MB 3.0 MB/s eta 0:01:17\n",
      "   ----------- ---------------------------- 96.5/326.9 MB 3.0 MB/s eta 0:01:16\n",
      "   ----------- ---------------------------- 97.5/326.9 MB 3.0 MB/s eta 0:01:17\n",
      "   ------------ --------------------------- 99.4/326.9 MB 3.1 MB/s eta 0:01:15\n",
      "   ------------ --------------------------- 100.7/326.9 MB 3.1 MB/s eta 0:01:14\n",
      "   ------------ --------------------------- 101.7/326.9 MB 3.1 MB/s eta 0:01:13\n",
      "   ------------ --------------------------- 102.8/326.9 MB 3.1 MB/s eta 0:01:12\n",
      "   ------------ --------------------------- 103.8/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------ --------------------------- 104.9/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------ --------------------------- 105.9/326.9 MB 3.2 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 107.0/326.9 MB 3.2 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 107.0/326.9 MB 3.2 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 107.0/326.9 MB 3.2 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 107.0/326.9 MB 3.2 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 108.0/326.9 MB 3.1 MB/s eta 0:01:12\n",
      "   ------------- -------------------------- 109.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 110.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 110.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 110.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 111.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 111.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 111.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 111.1/326.9 MB 3.1 MB/s eta 0:01:11\n",
      "   ------------- -------------------------- 113.2/326.9 MB 3.1 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 113.2/326.9 MB 3.1 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 113.2/326.9 MB 3.1 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 113.2/326.9 MB 3.1 MB/s eta 0:01:10\n",
      "   ------------- -------------------------- 114.3/326.9 MB 3.1 MB/s eta 0:01:10\n",
      "   -------------- ------------------------- 115.3/326.9 MB 3.1 MB/s eta 0:01:09\n",
      "   -------------- ------------------------- 115.3/326.9 MB 3.1 MB/s eta 0:01:09\n",
      "   -------------- ------------------------- 116.4/326.9 MB 3.1 MB/s eta 0:01:09\n",
      "   -------------- ------------------------- 117.4/326.9 MB 3.1 MB/s eta 0:01:08\n",
      "   -------------- ------------------------- 117.4/326.9 MB 3.1 MB/s eta 0:01:08\n",
      "   -------------- ------------------------- 118.5/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 119.5/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 119.5/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 119.5/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 119.5/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 119.5/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 120.6/326.9 MB 3.1 MB/s eta 0:01:08\n",
      "   -------------- ------------------------- 121.6/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 121.6/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   -------------- ------------------------- 121.6/326.9 MB 3.1 MB/s eta 0:01:07\n",
      "   --------------- ------------------------ 122.7/326.9 MB 3.1 MB/s eta 0:01:06\n",
      "   --------------- ------------------------ 124.0/326.9 MB 3.1 MB/s eta 0:01:05\n",
      "   --------------- ------------------------ 124.8/326.9 MB 3.2 MB/s eta 0:01:05\n",
      "   --------------- ------------------------ 125.8/326.9 MB 3.1 MB/s eta 0:01:04\n",
      "   --------------- ------------------------ 126.9/326.9 MB 3.1 MB/s eta 0:01:04\n",
      "   --------------- ------------------------ 126.9/326.9 MB 3.1 MB/s eta 0:01:04\n",
      "   --------------- ------------------------ 126.9/326.9 MB 3.1 MB/s eta 0:01:04\n",
      "   --------------- ------------------------ 126.9/326.9 MB 3.1 MB/s eta 0:01:04\n",
      "   --------------- ------------------------ 128.7/326.9 MB 3.1 MB/s eta 0:01:04\n",
      "   --------------- ------------------------ 130.0/326.9 MB 3.2 MB/s eta 0:01:03\n",
      "   --------------- ------------------------ 130.0/326.9 MB 3.2 MB/s eta 0:01:03\n",
      "   --------------- ------------------------ 130.0/326.9 MB 3.2 MB/s eta 0:01:03\n",
      "   --------------- ------------------------ 130.0/326.9 MB 3.2 MB/s eta 0:01:03\n",
      "   ---------------- ----------------------- 131.1/326.9 MB 3.2 MB/s eta 0:01:03\n",
      "   ---------------- ----------------------- 132.1/326.9 MB 3.2 MB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 132.1/326.9 MB 3.2 MB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 133.2/326.9 MB 3.2 MB/s eta 0:01:01\n",
      "   ---------------- ----------------------- 133.7/326.9 MB 3.2 MB/s eta 0:01:01\n",
      "   ---------------- ----------------------- 134.2/326.9 MB 3.2 MB/s eta 0:01:01\n",
      "   ---------------- ----------------------- 135.3/326.9 MB 3.2 MB/s eta 0:01:01\n",
      "   ---------------- ----------------------- 135.3/326.9 MB 3.2 MB/s eta 0:01:01\n",
      "   ---------------- ----------------------- 135.3/326.9 MB 3.2 MB/s eta 0:01:01\n",
      "   ---------------- ----------------------- 136.3/326.9 MB 3.1 MB/s eta 0:01:02\n",
      "   ---------------- ----------------------- 137.4/326.9 MB 3.2 MB/s eta 0:01:00\n",
      "   ---------------- ----------------------- 138.4/326.9 MB 3.2 MB/s eta 0:01:00\n",
      "   ----------------- ---------------------- 139.5/326.9 MB 3.2 MB/s eta 0:00:59\n",
      "   ----------------- ---------------------- 141.6/326.9 MB 3.2 MB/s eta 0:00:58\n",
      "   ----------------- ---------------------- 142.6/326.9 MB 3.2 MB/s eta 0:00:58\n",
      "   ----------------- ---------------------- 143.7/326.9 MB 3.2 MB/s eta 0:00:58\n",
      "   ----------------- ---------------------- 144.7/326.9 MB 3.2 MB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 144.7/326.9 MB 3.2 MB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 144.7/326.9 MB 3.2 MB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 144.7/326.9 MB 3.2 MB/s eta 0:00:57\n",
      "   ----------------- ---------------------- 145.8/326.9 MB 3.2 MB/s eta 0:00:58\n",
      "   ----------------- ---------------------- 145.8/326.9 MB 3.2 MB/s eta 0:00:58\n",
      "   ----------------- ---------------------- 146.8/326.9 MB 3.1 MB/s eta 0:00:58\n",
      "   ------------------ --------------------- 147.8/326.9 MB 3.1 MB/s eta 0:00:57\n",
      "   ------------------ --------------------- 147.8/326.9 MB 3.1 MB/s eta 0:00:57\n",
      "   ------------------ --------------------- 147.8/326.9 MB 3.1 MB/s eta 0:00:57\n",
      "   ------------------ --------------------- 148.9/326.9 MB 3.1 MB/s eta 0:00:58\n",
      "   ------------------ --------------------- 148.9/326.9 MB 3.1 MB/s eta 0:00:58\n",
      "   ------------------ --------------------- 149.9/326.9 MB 3.1 MB/s eta 0:00:57\n",
      "   ------------------ --------------------- 149.9/326.9 MB 3.1 MB/s eta 0:00:57\n",
      "   ------------------ --------------------- 151.0/326.9 MB 3.1 MB/s eta 0:00:57\n",
      "   ------------------ --------------------- 152.0/326.9 MB 3.1 MB/s eta 0:00:56\n",
      "   ------------------ --------------------- 153.1/326.9 MB 3.2 MB/s eta 0:00:56\n",
      "   ------------------ --------------------- 154.1/326.9 MB 3.2 MB/s eta 0:00:54\n",
      "   ------------------ --------------------- 154.1/326.9 MB 3.2 MB/s eta 0:00:54\n",
      "   ------------------ --------------------- 154.1/326.9 MB 3.2 MB/s eta 0:00:54\n",
      "   ------------------ --------------------- 154.1/326.9 MB 3.2 MB/s eta 0:00:54\n",
      "   ------------------ --------------------- 155.2/326.9 MB 3.1 MB/s eta 0:00:55\n",
      "   ------------------- -------------------- 156.2/326.9 MB 3.1 MB/s eta 0:00:55\n",
      "   ------------------- -------------------- 157.3/326.9 MB 3.2 MB/s eta 0:00:54\n",
      "   ------------------- -------------------- 158.3/326.9 MB 3.1 MB/s eta 0:00:54\n",
      "   ------------------- -------------------- 158.3/326.9 MB 3.1 MB/s eta 0:00:54\n",
      "   ------------------- -------------------- 158.3/326.9 MB 3.1 MB/s eta 0:00:54\n",
      "   ------------------- -------------------- 159.4/326.9 MB 3.1 MB/s eta 0:00:55\n",
      "   ------------------- -------------------- 159.4/326.9 MB 3.1 MB/s eta 0:00:55\n",
      "   ------------------- -------------------- 160.4/326.9 MB 3.1 MB/s eta 0:00:53\n",
      "   ------------------- -------------------- 161.5/326.9 MB 3.2 MB/s eta 0:00:53\n",
      "   ------------------- -------------------- 161.5/326.9 MB 3.2 MB/s eta 0:00:53\n",
      "   -------------------- ------------------- 163.6/326.9 MB 3.2 MB/s eta 0:00:52\n",
      "   -------------------- ------------------- 164.6/326.9 MB 3.2 MB/s eta 0:00:52\n",
      "   -------------------- ------------------- 164.6/326.9 MB 3.2 MB/s eta 0:00:52\n",
      "   -------------------- ------------------- 164.6/326.9 MB 3.2 MB/s eta 0:00:52\n",
      "   -------------------- ------------------- 164.6/326.9 MB 3.2 MB/s eta 0:00:52\n",
      "   -------------------- ------------------- 165.7/326.9 MB 3.2 MB/s eta 0:00:51\n",
      "   -------------------- ------------------- 167.5/326.9 MB 3.2 MB/s eta 0:00:50\n",
      "   -------------------- ------------------- 167.8/326.9 MB 3.2 MB/s eta 0:00:50\n",
      "   -------------------- ------------------- 167.8/326.9 MB 3.2 MB/s eta 0:00:50\n",
      "   -------------------- ------------------- 168.8/326.9 MB 3.2 MB/s eta 0:00:50\n",
      "   -------------------- ------------------- 170.9/326.9 MB 3.2 MB/s eta 0:00:49\n",
      "   -------------------- ------------------- 170.9/326.9 MB 3.2 MB/s eta 0:00:49\n",
      "   -------------------- ------------------- 170.9/326.9 MB 3.2 MB/s eta 0:00:49\n",
      "   -------------------- ------------------- 170.9/326.9 MB 3.2 MB/s eta 0:00:49\n",
      "   --------------------- ------------------ 172.0/326.9 MB 3.1 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 172.0/326.9 MB 3.1 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 172.0/326.9 MB 3.1 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 173.0/326.9 MB 3.0 MB/s eta 0:00:51\n",
      "   --------------------- ------------------ 174.1/326.9 MB 3.1 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 174.1/326.9 MB 3.1 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 174.1/326.9 MB 3.1 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 175.1/326.9 MB 3.0 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 175.1/326.9 MB 3.0 MB/s eta 0:00:50\n",
      "   --------------------- ------------------ 176.2/326.9 MB 3.1 MB/s eta 0:00:49\n",
      "   --------------------- ------------------ 177.2/326.9 MB 3.1 MB/s eta 0:00:49\n",
      "   --------------------- ------------------ 178.3/326.9 MB 3.1 MB/s eta 0:00:48\n",
      "   --------------------- ------------------ 179.3/326.9 MB 3.1 MB/s eta 0:00:47\n",
      "   --------------------- ------------------ 179.3/326.9 MB 3.1 MB/s eta 0:00:47\n",
      "   --------------------- ------------------ 179.3/326.9 MB 3.1 MB/s eta 0:00:47\n",
      "   ---------------------- ----------------- 180.4/326.9 MB 3.1 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 180.4/326.9 MB 3.1 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 180.4/326.9 MB 3.1 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 181.4/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 181.4/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 181.4/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 181.9/326.9 MB 2.9 MB/s eta 0:00:50\n",
      "   ---------------------- ----------------- 183.5/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 183.5/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 183.5/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 183.5/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 183.5/326.9 MB 3.0 MB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 184.0/326.9 MB 2.9 MB/s eta 0:00:50\n",
      "   ---------------------- ----------------- 185.6/326.9 MB 2.9 MB/s eta 0:00:49\n",
      "   ---------------------- ----------------- 185.6/326.9 MB 2.9 MB/s eta 0:00:49\n",
      "   ---------------------- ----------------- 185.6/326.9 MB 2.9 MB/s eta 0:00:49\n",
      "   ---------------------- ----------------- 185.6/326.9 MB 2.9 MB/s eta 0:00:49\n",
      "   ---------------------- ----------------- 186.6/326.9 MB 2.8 MB/s eta 0:00:51\n",
      "   ---------------------- ----------------- 187.7/326.9 MB 2.8 MB/s eta 0:00:50\n",
      "   ----------------------- ---------------- 188.7/326.9 MB 2.8 MB/s eta 0:00:50\n",
      "   ----------------------- ---------------- 188.7/326.9 MB 2.8 MB/s eta 0:00:50\n",
      "   ----------------------- ---------------- 188.7/326.9 MB 2.8 MB/s eta 0:00:50\n",
      "   ----------------------- ---------------- 189.8/326.9 MB 2.8 MB/s eta 0:00:50\n",
      "   ----------------------- ---------------- 190.8/326.9 MB 2.8 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 191.9/326.9 MB 2.8 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 191.9/326.9 MB 2.8 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 191.9/326.9 MB 2.8 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 192.9/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 192.9/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 192.9/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 192.9/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 194.0/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 194.0/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 194.0/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 194.0/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 194.0/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 195.0/326.9 MB 2.7 MB/s eta 0:00:49\n",
      "   ----------------------- ---------------- 196.1/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 197.1/326.9 MB 2.8 MB/s eta 0:00:48\n",
      "   ------------------------ --------------- 198.2/326.9 MB 2.8 MB/s eta 0:00:47\n",
      "   ------------------------ --------------- 198.2/326.9 MB 2.8 MB/s eta 0:00:47\n",
      "   ------------------------ --------------- 198.2/326.9 MB 2.8 MB/s eta 0:00:47\n",
      "   ------------------------ --------------- 199.2/326.9 MB 2.7 MB/s eta 0:00:47\n",
      "   ------------------------ --------------- 200.3/326.9 MB 2.8 MB/s eta 0:00:46\n",
      "   ------------------------ --------------- 200.3/326.9 MB 2.8 MB/s eta 0:00:46\n",
      "   ------------------------ --------------- 200.3/326.9 MB 2.8 MB/s eta 0:00:46\n",
      "   ------------------------ --------------- 200.3/326.9 MB 2.8 MB/s eta 0:00:46\n",
      "   ------------------------ --------------- 201.3/326.9 MB 2.8 MB/s eta 0:00:46\n",
      "   ------------------------ --------------- 202.4/326.9 MB 2.8 MB/s eta 0:00:45\n",
      "   ------------------------ --------------- 202.4/326.9 MB 2.8 MB/s eta 0:00:45\n",
      "   ------------------------ --------------- 202.4/326.9 MB 2.8 MB/s eta 0:00:45\n",
      "   ------------------------ --------------- 203.4/326.9 MB 2.8 MB/s eta 0:00:45\n",
      "   ------------------------- -------------- 204.5/326.9 MB 2.8 MB/s eta 0:00:45\n",
      "   ------------------------- -------------- 204.5/326.9 MB 2.8 MB/s eta 0:00:45\n",
      "   ------------------------- -------------- 204.5/326.9 MB 2.8 MB/s eta 0:00:45\n",
      "   ------------------------- -------------- 205.5/326.9 MB 2.7 MB/s eta 0:00:46\n",
      "   ------------------------- -------------- 205.5/326.9 MB 2.7 MB/s eta 0:00:46\n",
      "   ------------------------- -------------- 206.6/326.9 MB 2.7 MB/s eta 0:00:45\n",
      "   ------------------------- -------------- 207.6/326.9 MB 2.7 MB/s eta 0:00:44\n",
      "   ------------------------- -------------- 207.6/326.9 MB 2.7 MB/s eta 0:00:44\n",
      "   ------------------------- -------------- 207.6/326.9 MB 2.7 MB/s eta 0:00:44\n",
      "   ------------------------- -------------- 208.7/326.9 MB 2.7 MB/s eta 0:00:44\n",
      "   ------------------------- -------------- 209.7/326.9 MB 2.7 MB/s eta 0:00:44\n",
      "   ------------------------- -------------- 209.7/326.9 MB 2.7 MB/s eta 0:00:44\n",
      "   ------------------------- -------------- 210.8/326.9 MB 2.7 MB/s eta 0:00:43\n",
      "   ------------------------- -------------- 211.8/326.9 MB 2.7 MB/s eta 0:00:43\n",
      "   ------------------------- -------------- 211.8/326.9 MB 2.7 MB/s eta 0:00:43\n",
      "   ------------------------- -------------- 211.8/326.9 MB 2.7 MB/s eta 0:00:43\n",
      "   ------------------------- -------------- 211.8/326.9 MB 2.7 MB/s eta 0:00:43\n",
      "   ------------------------- -------------- 211.8/326.9 MB 2.7 MB/s eta 0:00:43\n",
      "   -------------------------- ------------- 212.9/326.9 MB 2.6 MB/s eta 0:00:44\n",
      "   -------------------------- ------------- 212.9/326.9 MB 2.6 MB/s eta 0:00:44\n",
      "   -------------------------- ------------- 213.9/326.9 MB 2.7 MB/s eta 0:00:43\n",
      "   -------------------------- ------------- 215.0/326.9 MB 2.7 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 216.0/326.9 MB 2.7 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 216.0/326.9 MB 2.7 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 216.0/326.9 MB 2.7 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 216.5/326.9 MB 2.6 MB/s eta 0:00:43\n",
      "   -------------------------- ------------- 218.1/326.9 MB 2.6 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 218.1/326.9 MB 2.6 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 218.1/326.9 MB 2.6 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 218.1/326.9 MB 2.6 MB/s eta 0:00:42\n",
      "   -------------------------- ------------- 219.2/326.9 MB 2.5 MB/s eta 0:00:43\n",
      "   -------------------------- ------------- 220.2/326.9 MB 2.6 MB/s eta 0:00:42\n",
      "   --------------------------- ------------ 221.2/326.9 MB 2.6 MB/s eta 0:00:41\n",
      "   --------------------------- ------------ 222.3/326.9 MB 2.6 MB/s eta 0:00:41\n",
      "   --------------------------- ------------ 223.3/326.9 MB 2.6 MB/s eta 0:00:40\n",
      "   --------------------------- ------------ 223.3/326.9 MB 2.6 MB/s eta 0:00:40\n",
      "   --------------------------- ------------ 223.3/326.9 MB 2.6 MB/s eta 0:00:40\n",
      "   --------------------------- ------------ 223.6/326.9 MB 2.6 MB/s eta 0:00:41\n",
      "   --------------------------- ------------ 224.4/326.9 MB 2.6 MB/s eta 0:00:40\n",
      "   --------------------------- ------------ 224.4/326.9 MB 2.6 MB/s eta 0:00:40\n",
      "   --------------------------- ------------ 225.4/326.9 MB 2.6 MB/s eta 0:00:40\n",
      "   --------------------------- ------------ 226.5/326.9 MB 2.6 MB/s eta 0:00:39\n",
      "   --------------------------- ------------ 227.5/326.9 MB 2.6 MB/s eta 0:00:39\n",
      "   --------------------------- ------------ 228.6/326.9 MB 2.6 MB/s eta 0:00:38\n",
      "   --------------------------- ------------ 228.6/326.9 MB 2.6 MB/s eta 0:00:38\n",
      "   ---------------------------- ----------- 229.6/326.9 MB 2.6 MB/s eta 0:00:38\n",
      "   ---------------------------- ----------- 230.7/326.9 MB 2.6 MB/s eta 0:00:37\n",
      "   ---------------------------- ----------- 231.7/326.9 MB 2.6 MB/s eta 0:00:37\n",
      "   ---------------------------- ----------- 231.7/326.9 MB 2.6 MB/s eta 0:00:37\n",
      "   ---------------------------- ----------- 232.8/326.9 MB 2.6 MB/s eta 0:00:36\n",
      "   ---------------------------- ----------- 233.8/326.9 MB 2.7 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 233.8/326.9 MB 2.7 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 234.9/326.9 MB 2.7 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 235.9/326.9 MB 2.6 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 235.9/326.9 MB 2.6 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 235.9/326.9 MB 2.6 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 235.9/326.9 MB 2.6 MB/s eta 0:00:35\n",
      "   ---------------------------- ----------- 237.0/326.9 MB 2.6 MB/s eta 0:00:35\n",
      "   ----------------------------- ---------- 238.0/326.9 MB 2.6 MB/s eta 0:00:34\n",
      "   ----------------------------- ---------- 239.3/326.9 MB 2.6 MB/s eta 0:00:34\n",
      "   ----------------------------- ---------- 241.2/326.9 MB 2.7 MB/s eta 0:00:33\n",
      "   ----------------------------- ---------- 242.2/326.9 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 242.5/326.9 MB 2.6 MB/s eta 0:00:33\n",
      "   ----------------------------- ---------- 243.3/326.9 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 243.3/326.9 MB 2.7 MB/s eta 0:00:32\n",
      "   ----------------------------- ---------- 244.3/326.9 MB 2.7 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 245.4/326.9 MB 2.7 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 245.4/326.9 MB 2.7 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 246.4/326.9 MB 2.7 MB/s eta 0:00:31\n",
      "   ------------------------------ --------- 247.5/326.9 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 247.5/326.9 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 247.5/326.9 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 247.5/326.9 MB 2.7 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 248.5/326.9 MB 2.6 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 249.6/326.9 MB 2.6 MB/s eta 0:00:30\n",
      "   ------------------------------ --------- 250.6/326.9 MB 2.7 MB/s eta 0:00:29\n",
      "   ------------------------------ --------- 251.7/326.9 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------------ --------- 252.7/326.9 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------------ --------- 252.7/326.9 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------------ --------- 252.7/326.9 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------------ --------- 252.7/326.9 MB 2.7 MB/s eta 0:00:28\n",
      "   ------------------------------- -------- 254.8/326.9 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 254.8/326.9 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 254.8/326.9 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 254.8/326.9 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 255.9/326.9 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 255.9/326.9 MB 2.7 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 256.9/326.9 MB 2.6 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 257.9/326.9 MB 2.6 MB/s eta 0:00:27\n",
      "   ------------------------------- -------- 259.0/326.9 MB 2.7 MB/s eta 0:00:26\n",
      "   ------------------------------- -------- 260.0/326.9 MB 2.7 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 260.0/326.9 MB 2.7 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 260.8/326.9 MB 2.7 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 261.1/326.9 MB 2.7 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 261.1/326.9 MB 2.7 MB/s eta 0:00:25\n",
      "   ------------------------------- -------- 261.1/326.9 MB 2.7 MB/s eta 0:00:25\n",
      "   -------------------------------- ------- 262.1/326.9 MB 2.7 MB/s eta 0:00:24\n",
      "   -------------------------------- ------- 263.7/326.9 MB 2.7 MB/s eta 0:00:24\n",
      "   -------------------------------- ------- 264.2/326.9 MB 2.8 MB/s eta 0:00:23\n",
      "   -------------------------------- ------- 265.3/326.9 MB 2.8 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 266.3/326.9 MB 2.8 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 266.3/326.9 MB 2.8 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 267.4/326.9 MB 2.8 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 267.4/326.9 MB 2.8 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 267.4/326.9 MB 2.8 MB/s eta 0:00:22\n",
      "   -------------------------------- ------- 268.4/326.9 MB 2.8 MB/s eta 0:00:21\n",
      "   -------------------------------- ------- 269.5/326.9 MB 2.8 MB/s eta 0:00:21\n",
      "   --------------------------------- ------ 270.5/326.9 MB 2.8 MB/s eta 0:00:20\n",
      "   --------------------------------- ------ 270.5/326.9 MB 2.8 MB/s eta 0:00:20\n",
      "   --------------------------------- ------ 270.5/326.9 MB 2.8 MB/s eta 0:00:20\n",
      "   --------------------------------- ------ 271.6/326.9 MB 2.8 MB/s eta 0:00:21\n",
      "   --------------------------------- ------ 271.6/326.9 MB 2.8 MB/s eta 0:00:21\n",
      "   --------------------------------- ------ 272.6/326.9 MB 2.8 MB/s eta 0:00:20\n",
      "   --------------------------------- ------ 272.6/326.9 MB 2.8 MB/s eta 0:00:20\n",
      "   --------------------------------- ------ 274.7/326.9 MB 2.8 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 274.7/326.9 MB 2.8 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 274.7/326.9 MB 2.8 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 275.8/326.9 MB 2.8 MB/s eta 0:00:19\n",
      "   --------------------------------- ------ 276.8/326.9 MB 2.8 MB/s eta 0:00:18\n",
      "   ---------------------------------- ----- 277.9/326.9 MB 2.9 MB/s eta 0:00:18\n",
      "   ---------------------------------- ----- 277.9/326.9 MB 2.9 MB/s eta 0:00:18\n",
      "   ---------------------------------- ----- 278.9/326.9 MB 2.9 MB/s eta 0:00:17\n",
      "   ---------------------------------- ----- 280.0/326.9 MB 2.9 MB/s eta 0:00:17\n",
      "   ---------------------------------- ----- 281.0/326.9 MB 3.0 MB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 282.1/326.9 MB 3.0 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 283.1/326.9 MB 3.0 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 284.2/326.9 MB 3.0 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 284.2/326.9 MB 3.0 MB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 285.2/326.9 MB 3.0 MB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 286.3/326.9 MB 3.0 MB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 287.3/326.9 MB 3.0 MB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 287.3/326.9 MB 3.0 MB/s eta 0:00:14\n",
      "   ----------------------------------- ---- 288.4/326.9 MB 3.0 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 289.4/326.9 MB 3.0 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 289.4/326.9 MB 3.0 MB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 290.5/326.9 MB 3.1 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 290.5/326.9 MB 3.1 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 290.5/326.9 MB 3.1 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 291.5/326.9 MB 3.0 MB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 292.6/326.9 MB 3.0 MB/s eta 0:00:12\n",
      "   ------------------------------------ --- 294.6/326.9 MB 3.1 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 295.7/326.9 MB 3.1 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 295.7/326.9 MB 3.1 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 295.7/326.9 MB 3.1 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 295.7/326.9 MB 3.1 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 295.7/326.9 MB 3.1 MB/s eta 0:00:11\n",
      "   ------------------------------------ --- 296.7/326.9 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 297.8/326.9 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 298.8/326.9 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------------------------------ --- 299.9/326.9 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 300.9/326.9 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 300.9/326.9 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------------------ --- 301.5/326.9 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------------------------------- -- 303.0/326.9 MB 3.1 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 304.1/326.9 MB 3.2 MB/s eta 0:00:08\n",
      "   ------------------------------------- -- 305.1/326.9 MB 3.2 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 305.1/326.9 MB 3.2 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 305.4/326.9 MB 3.2 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 306.2/326.9 MB 3.2 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 307.2/326.9 MB 3.2 MB/s eta 0:00:07\n",
      "   ------------------------------------- -- 308.3/326.9 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 309.3/326.9 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 309.3/326.9 MB 3.3 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 310.4/326.9 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 310.4/326.9 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 310.4/326.9 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 310.4/326.9 MB 3.2 MB/s eta 0:00:06\n",
      "   ------------------------------------- -- 310.4/326.9 MB 3.2 MB/s eta 0:00:06\n",
      "   -------------------------------------- - 311.4/326.9 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 311.4/326.9 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 312.5/326.9 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 313.5/326.9 MB 3.2 MB/s eta 0:00:05\n",
      "   -------------------------------------- - 314.6/326.9 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 314.6/326.9 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 314.6/326.9 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 314.6/326.9 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 315.6/326.9 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 315.6/326.9 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 316.7/326.9 MB 3.2 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 317.7/326.9 MB 3.2 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 317.7/326.9 MB 3.2 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 317.7/326.9 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  318.8/326.9 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  318.8/326.9 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  318.8/326.9 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  318.8/326.9 MB 3.2 MB/s eta 0:00:03\n",
      "   ---------------------------------------  319.8/326.9 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------------------  320.9/326.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  320.9/326.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  320.9/326.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  321.9/326.9 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------------------------  323.0/326.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------------------------  324.0/326.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  324.0/326.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  325.1/326.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  326.1/326.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  326.6/326.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  326.6/326.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 326.9/326.9 MB 3.1 MB/s eta 0:00:00\n",
      "Downloading tb_nightly-2.19.0a20250218-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 5.5/5.5 MB 47.6 MB/s eta 0:00:00\n",
      "Downloading keras_nightly-3.10.0.dev2025060503-py3-none-any.whl (1.4 MB)\n",
      "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.4/1.4 MB 35.0 MB/s eta 0:00:00\n",
      "Installing collected packages: tb-nightly, keras-nightly, tf-nightly\n",
      "\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ---------------------------------------- 0/3 [tb-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   ------------- -------------------------- 1/3 [keras-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   -------------------------- ------------- 2/3 [tf-nightly]\n",
      "   ---------------------------------------- 3/3 [tf-nightly]\n",
      "\n",
      "Successfully installed keras-nightly-3.10.0.dev2025060503 tb-nightly-2.19.0a20250218 tf-nightly-2.20.0.dev20250605\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e6366f-dde8-40c1-9a0f-de6aa456e27c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b775c88d-5b53-458c-a1c1-a2d40b0624e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: tensorflow<2.20,>=2.19 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tf-keras) (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (5.29.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (57.4.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.72.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow<2.20,>=2.19->tf-keras) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.0)\n",
      "Requirement already satisfied: optree in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 13.4 MB/s eta 0:00:00\n",
      "Installing collected packages: tf-keras\n",
      "Successfully installed tf-keras-2.19.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064a1b69-205d-4a58-9b49-c63c8ff0484e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub[hf_xet] in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (0.32.4)\n",
      "Requirement already satisfied: filelock in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (6.0.2)\n",
      "Requirement already satisfied: requests in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface_hub[hf_xet]) (4.13.2)\n",
      "Collecting hf-xet<2.0.0,>=1.1.2 (from huggingface_hub[hf_xet])\n",
      "  Downloading hf_xet-1.1.3-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Requirement already satisfied: colorama in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub[hf_xet]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->huggingface_hub[hf_xet]) (2025.4.26)\n",
      "Downloading hf_xet-1.1.3-cp37-abi3-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.3/2.3 MB 18.8 MB/s eta 0:00:00\n",
      "Installing collected packages: hf-xet\n",
      "Successfully installed hf-xet-1.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub[hf_xet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1aca4c8-1d48-4863-8d1d-584d59a93c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
      "\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'entity': 'I-PER',\n",
       "  'score': np.float32(0.9903374),\n",
       "  'index': 5,\n",
       "  'word': 'V',\n",
       "  'start': np.int32(10),\n",
       "  'end': np.int32(11)},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.86455655),\n",
       "  'index': 6,\n",
       "  'word': '##id',\n",
       "  'start': np.int32(11),\n",
       "  'end': np.int32(13)},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.80260736),\n",
       "  'index': 7,\n",
       "  'word': '##ush',\n",
       "  'start': np.int32(13),\n",
       "  'end': np.int32(16)},\n",
       " {'entity': 'I-PER',\n",
       "  'score': np.float32(0.94455975),\n",
       "  'index': 8,\n",
       "  'word': '##an',\n",
       "  'start': np.int32(16),\n",
       "  'end': np.int32(18)},\n",
       " {'entity': 'I-LOC',\n",
       "  'score': np.float32(0.9998109),\n",
       "  'index': 13,\n",
       "  'word': 'Japan',\n",
       "  'start': np.int32(33),\n",
       "  'end': np.int32(38)}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NER\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline('ner')\n",
    "classifier(\"Hello I'm Vidushan and I live in Japan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8b472934-f022-425f-bc5d-615ad9f90e53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertForTokenClassification: ['bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
      "Device set to use 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'entity': 'INTJ', 'score': np.float32(0.99684346), 'index': 1, 'word': 'hello', 'start': np.int32(0), 'end': np.int32(5)}, {'entity': 'PRON', 'score': np.float32(0.9994294), 'index': 2, 'word': 'i', 'start': np.int32(6), 'end': np.int32(7)}, {'entity': 'AUX', 'score': np.float32(0.99703026), 'index': 3, 'word': \"'\", 'start': np.int32(7), 'end': np.int32(8)}, {'entity': 'AUX', 'score': np.float32(0.9963716), 'index': 4, 'word': 'm', 'start': np.int32(8), 'end': np.int32(9)}, {'entity': 'PROPN', 'score': np.float32(0.99798965), 'index': 5, 'word': 'vi', 'start': np.int32(10), 'end': np.int32(12)}, {'entity': 'PROPN', 'score': np.float32(0.991126), 'index': 6, 'word': '##dus', 'start': np.int32(12), 'end': np.int32(15)}, {'entity': 'PROPN', 'score': np.float32(0.99583024), 'index': 7, 'word': '##han', 'start': np.int32(15), 'end': np.int32(18)}, {'entity': 'CCONJ', 'score': np.float32(0.999213), 'index': 8, 'word': 'and', 'start': np.int32(19), 'end': np.int32(22)}, {'entity': 'PRON', 'score': np.float32(0.9994771), 'index': 9, 'word': 'i', 'start': np.int32(23), 'end': np.int32(24)}, {'entity': 'VERB', 'score': np.float32(0.9985649), 'index': 10, 'word': 'live', 'start': np.int32(25), 'end': np.int32(29)}, {'entity': 'ADP', 'score': np.float32(0.99942744), 'index': 11, 'word': 'in', 'start': np.int32(30), 'end': np.int32(32)}, {'entity': 'PROPN', 'score': np.float32(0.99877924), 'index': 12, 'word': 'japan', 'start': np.int32(33), 'end': np.int32(38)}, {'entity': 'PUNCT', 'score': np.float32(0.9996561), 'index': 13, 'word': '.', 'start': np.int32(38), 'end': np.int32(39)}]\n"
     ]
    }
   ],
   "source": [
    "#POS\n",
    "from transformers import pipelines\n",
    "\n",
    "classifiers = pipeline(\"token-classification\", \n",
    "                       model=\"vblagoje/bert-english-uncased-finetuned-pos\")\n",
    "result = classifiers(\"hello I'm Vidushan and I live in Japan.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b914e-8adb-4ac0-886f-9e84bbe0ed26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4947ff5a-8da0-488d-b403-f1ce0b50ba3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (4.52.4)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting seqeval\n",
      "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: filelock in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (0.32.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-20.0.0-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pandas (from datasets)\n",
      "  Downloading pandas-2.3.0-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.30.0->transformers)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.9-cp310-cp310-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting scikit-learn>=0.21.3 (from seqeval)\n",
      "  Downloading scikit_learn-1.7.0-cp310-cp310-win_amd64.whl.metadata (14 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.6.2-cp310-cp310-win_amd64.whl.metadata (17 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.4.4-cp310-cp310-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.1-cp310-cp310-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.0-cp310-cp310-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from requests->transformers) (2025.4.26)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.21.3->seqeval)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas->datasets)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas->datasets)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Downloading aiohttp-3.12.9-cp310-cp310-win_amd64.whl (449 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Downloading multidict-6.4.4-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Using cached yarl-1.20.0-cp310-cp310-win_amd64.whl (92 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.6.2-cp310-cp310-win_amd64.whl (45 kB)\n",
      "Using cached propcache-0.3.1-cp310-cp310-win_amd64.whl (45 kB)\n",
      "Using cached pyarrow-20.0.0-cp310-cp310-win_amd64.whl (25.8 MB)\n",
      "Downloading scikit_learn-1.7.0-cp310-cp310-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 10.7/10.7 MB 74.1 MB/s eta 0:00:00\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached scipy-1.15.3-cp310-cp310-win_amd64.whl (41.3 MB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading pandas-2.3.0-cp310-cp310-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 11.1/11.1 MB 86.6 MB/s eta 0:00:00\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached xxhash-3.5.0-cp310-cp310-win_amd64.whl (30 kB)\n",
      "Building wheels for collected packages: seqeval\n",
      "  Building wheel for seqeval (setup.py): started\n",
      "  Building wheel for seqeval (setup.py): finished with status 'done'\n",
      "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16205 sha256=fdba0bc745cdcc80f0607aa4e2a861c5d0c87d2b6073820c706999b25e4526b1\n",
      "  Stored in directory: c:\\users\\ymtla\\appdata\\local\\pip\\cache\\wheels\\1a\\67\\4a\\ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\n",
      "Successfully built seqeval\n",
      "Installing collected packages: pytz, xxhash, tzdata, threadpoolctl, scipy, pyarrow, propcache, multidict, joblib, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, scikit-learn, pandas, multiprocess, aiosignal, seqeval, aiohttp, datasets, evaluate\n",
      "\n",
      "   ----------------------------------------  0/23 [pytz]\n",
      "   --- ------------------------------------  2/23 [tzdata]\n",
      "   --- ------------------------------------  2/23 [tzdata]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   ------ ---------------------------------  4/23 [scipy]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   -------- -------------------------------  5/23 [pyarrow]\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "  Attempting uninstall: fsspec\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "   ------------- --------------------------  8/23 [joblib]\n",
      "   --------------- ------------------------  9/23 [fsspec]\n",
      "   --------------- ------------------------  9/23 [fsspec]\n",
      "   --------------- ------------------------  9/23 [fsspec]\n",
      "   --------------- ------------------------  9/23 [fsspec]\n",
      "   ------------------- -------------------- 11/23 [dill]\n",
      "   ------------------- -------------------- 11/23 [dill]\n",
      "   ------------------------ --------------- 14/23 [yarl]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   -------------------------- ------------- 15/23 [scikit-learn]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   --------------------------- ------------ 16/23 [pandas]\n",
      "   ----------------------------- ---------- 17/23 [multiprocess]\n",
      "   ----------------------------- ---------- 17/23 [multiprocess]\n",
      "   ---------------------------------- ----- 20/23 [aiohttp]\n",
      "   ---------------------------------- ----- 20/23 [aiohttp]\n",
      "   ---------------------------------- ----- 20/23 [aiohttp]\n",
      "   ------------------------------------ --- 21/23 [datasets]\n",
      "   ------------------------------------ --- 21/23 [datasets]\n",
      "   ------------------------------------ --- 21/23 [datasets]\n",
      "   ------------------------------------ --- 21/23 [datasets]\n",
      "   ------------------------------------ --- 21/23 [datasets]\n",
      "   ------------------------------------ --- 21/23 [datasets]\n",
      "   ------------------------------------ --- 21/23 [datasets]\n",
      "   -------------------------------------- - 22/23 [evaluate]\n",
      "   ---------------------------------------- 23/23 [evaluate]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.9 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.6.0 dill-0.3.8 evaluate-0.4.3 frozenlist-1.6.2 fsspec-2025.3.0 joblib-1.5.1 multidict-6.4.4 multiprocess-0.70.16 pandas-2.3.0 propcache-0.3.1 pyarrow-20.0.0 pytz-2025.2 scikit-learn-1.7.0 scipy-1.15.3 seqeval-1.2.2 threadpoolctl-3.6.0 tzdata-2025.2 xxhash-3.5.0 yarl-1.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'seqeval' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'seqeval'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets evaluate seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8905b32-8023-4809-9538-8c69dccd37eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bef5368f-01c7-485c-aeee-3072deaf6c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fintune Disilbert on the WNUT 17 dataset to the detect new entities\n"
     ]
    }
   ],
   "source": [
    "print(\"Fintune Disilbert on the WNUT 17 dataset to the detect new entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f2d988-e3df-4471-b5bd-a17e461a3d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6450879c-7745-4ab8-ba27-4ecd2e3aab11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d2e1b19e4d42038173394becefa896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b463a981-f514-4024-b817-b2cc837efc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Using cached ipywidgets-8.1.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: ipython>=6.1.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipywidgets) (8.36.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Using cached widgetsnbextension-4.0.14-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: colorama in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: decorator in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.51)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (2.19.1)\n",
      "Requirement already satisfied: stack_data in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from ipython>=6.1.0->ipywidgets) (4.13.2)\n",
      "Requirement already satisfied: wcwidth in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in d:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages (from stack_data->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Using cached ipywidgets-8.1.7-py3-none-any.whl (139 kB)\n",
      "Using cached jupyterlab_widgets-3.0.15-py3-none-any.whl (216 kB)\n",
      "Using cached widgetsnbextension-4.0.14-py3-none-any.whl (2.2 MB)\n",
      "Installing collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   -------------------------- ------------- 2/3 [ipywidgets]\n",
      "   ---------------------------------------- 3/3 [ipywidgets]\n",
      "\n",
      "Successfully installed ipywidgets-8.1.7 jupyterlab_widgets-3.0.15 widgetsnbextension-4.0.14\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0e0ad0-27a6-4ad2-ab68-c3ae9c1fd28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|| 494k/494k [00:00<00:00, 18.5MB/s]\n",
      "Downloading data: 100%|| 115k/115k [00:00<00:00, 115MB/s]\n",
      "Downloading data: 100%|| 192k/192k [00:00<00:00, 191MB/s]\n",
      "Generating train split: 100%|| 3394/3394 [00:00<00:00, 17099.11 examples/s]\n",
      "Generating validation split: 100%|| 1009/1009 [00:00<00:00, 18342.89 examples/s]\n",
      "Generating test split: 100%|| 1287/1287 [00:00<00:00, 16674.14 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\",trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a601e689-47a8-4274-aed5-801348537912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'tokens': ['From',\n",
       "  'Green',\n",
       "  'Newsfeed',\n",
       "  ':',\n",
       "  'AHFA',\n",
       "  'extends',\n",
       "  'deadline',\n",
       "  'for',\n",
       "  'Sage',\n",
       "  'Award',\n",
       "  'to',\n",
       "  'Nov',\n",
       "  '.',\n",
       "  '5',\n",
       "  'http://tinyurl.com/24agj38'],\n",
       " 'ner_tags': [0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut[\"train\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "257b1641-20e7-4230-91c5-f96f39d11967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'B-corporation', 'I-corporation', 'B-creative-work', 'I-creative-work', 'B-group', 'I-group', 'B-location', 'I-location', 'B-person', 'I-person', 'B-product', 'I-product']\n"
     ]
    }
   ],
   "source": [
    "# This shows you the mapping from integers to actual label names\n",
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "print(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2143eafb-04ce-4de2-96e2-f20dd93e5629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: O\n",
      "Green: O\n",
      "Newsfeed: O\n",
      ":: O\n",
      "AHFA: B-group\n",
      "extends: O\n",
      "deadline: O\n",
      "for: O\n",
      "Sage: O\n",
      "Award: O\n",
      "to: O\n",
      "Nov: O\n",
      ".: O\n",
      "5: O\n",
      "http://tinyurl.com/24agj38: O\n"
     ]
    }
   ],
   "source": [
    "example = wnut[\"train\"][1]  # First training example\n",
    "\n",
    "tokens = example[\"tokens\"]\n",
    "tag_ids = example[\"ner_tags\"]\n",
    "\n",
    "# Convert each tag id to its string label\n",
    "tags = [label_list[tag_id] for tag_id in tag_ids]\n",
    "\n",
    "# Combine tokens and tags for display\n",
    "for token, tag in zip(tokens, tags):\n",
    "    print(f\"{token}: {tag}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "561a16e5-77a6-48ff-95ec-d97dc91d5085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load a Distilbert tokenizer to preprocess the tokens\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "230a1a91-4548-4223-9e41-87ae4af844e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de8056e4-62e5-4261-a3a5-6121a029cb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBertTokenizerFast(name_or_path='distilbert/distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
      "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a44c8060-1138-4184-b707-511da28332dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'paul',\n",
       " '##walk',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'empire',\n",
       " 'state',\n",
       " 'building',\n",
       " '=',\n",
       " 'es',\n",
       " '##b',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To tokenize the words into subwords, set is_split_into_words=True\n",
    "example = wnut[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "07a21f2f-3a82-45b8-8432-b6d6ebf03ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels=[]\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"label\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c67ef5aa-8181-40da-82f4-59ed75e2588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 3394/3394 [00:00<00:00, 14148.90 examples/s]\n",
      "Map: 100%|| 1009/1009 [00:00<00:00, 14386.17 examples/s]\n",
      "Map: 100%|| 1287/1287 [00:00<00:00, 13331.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "#To apply the preprocessing functionover the entire dataset, use datasets map function and to speed up the map function by setting batched =True.\n",
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "75560dfc-15f5-484a-8b3d-64c2ba6a8b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dynamically pad the sentences to the longest length in a batch during collection, instead of padding the whole data set to the maximum length.\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e9e162d-c28c-40b5-84c5-8b26c052405f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'label'])\n"
     ]
    }
   ],
   "source": [
    "# Each example in dataset must have the same-length 'input_ids' and 'labels'\n",
    "print(tokenized_wnut[\"train\"][0].keys())\n",
    "# Should output: dict_keys(['input_ids', 'attention_mask', 'labels', ...])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "414d143a-e0be-44f8-a4b2-fb47e4f02e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d9b336cf-7026-42fa-abb0-375e4f68588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|| 6.34k/6.34k [00:00<?, ?B/s]\n"
     ]
    }
   ],
   "source": [
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "66dcdb8c-7ccb-41e9-b656-856526c53748",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b9141e68-132e-4bfe-b4bd-b8d1dc249dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting labels from the example using the NER tags\n",
    "labels = [label_list[i] for i in example[\"ner_tags\"]]\n",
    "\n",
    "def compute_metrics(p):\n",
    "    # Unpacking predictions and labels\n",
    "    predictions, labels = p\n",
    "    \n",
    "    # Getting the predicted labels by taking the argmax\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Filtering true predictions to exclude labels with -100\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Filtering true labels to exclude labels with -100\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    # Computing evaluation metrics using seqeval\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    \n",
    "    # Returning the calculated metrics\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "801d6165-36b8-47ad-bdd5-1802a963c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a8e3e4ff-2842-4697-a595-ee3cbceeeafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label={\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "         }\n",
    "label2id={\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d9944466-21cd-4166-9dc9-2fa337a2e8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import create_optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "35498eeb-e3c9-41b0-9c97-e51f5148f246",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7ea9316d-b6a0-4b08-a847-666ad9e34cf3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForTokenClassification, TrainingArguments, Trainer\n\u001b[1;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForTokenClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert/distilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m13\u001b[39m, id2label\u001b[38;5;241m=\u001b[39mid2label, label2id\u001b[38;5;241m=\u001b[39mlabel2id\n\u001b[0;32m      5\u001b[0m )\n",
      "File \u001b[1;32mD:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1885\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   1883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 1885\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\utils\\import_utils.py:1854\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m is_tf_available():\n\u001b[1;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(PYTORCH_IMPORT_ERROR_WITH_TF\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   1856\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n",
      "\u001b[1;31mImportError\u001b[0m: \nAutoModelForTokenClassification requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForTokenClassification\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "01027605-ac2f-48cf-915a-c924289d7f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "num_train_epoches = 3\n",
    "num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epoches\n",
    "optimizer, lr_scedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=0.01,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fdb0a6d5-7444-44f1-9d4e-638a2b26f1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForTokenClassification: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForTokenClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForTokenClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForTokenClassification\n",
    "\n",
    "model = TFAutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "194e5125-86f2-4a08-a02b-0fda67bccf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_wnut[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_wnut[\"validation\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4d0f71c6-fd0f-44c8-bb48-a3fbe34ee2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(optimizer=optimizer)  # No loss argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9c0c2f67-285d-4e70-96d9-978a87122b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback\n",
    "\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72b6a547-e401-43fa-a417-88e0b6098f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\otake\\gpu-service\\users\\vidzshan\\notebooks\\HuggingFace\\Vidzshan\\my_awesome_eli5_clm-model is already a clone of https://huggingface.co/Vidzshan/my_awesome_eli5_clm-model. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    }
   ],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"Vidzshan/my_awesome_eli5_clm-model\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c7329e20-6bf5-4b5f-97f0-18c2699e6293",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4d39de62-6bfa-4a61-9a65-7504caa563a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node EagerPyFunc defined at (most recent call last):\n<stack traces unavailable>\nError in user-defined function passed to MapDataset:16 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2::ParallelMapV2::Map: ValueError: setting an array element with a sequence.\nTraceback (most recent call last):\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 267, in __call__\n    return func(device, token, args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 145, in __call__\n    outputs = self._call(device, args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 152, in _call\n    ret = self._func(*args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\datasets\\utils\\tf_utils.py\", line 99, in np_get_batch\n    batch = collate_fn(batch, **collate_fn_args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 44, in __call__\n    return self.tf_call(features)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 395, in tf_call\n    batch = {k: tf.convert_to_tensor(v, dtype=tf.int64) for k, v in batch.items()}\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 395, in <dictcomp>\n    batch = {k: tf.convert_to_tensor(v, dtype=tf.int64) for k, v in batch.items()}\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: setting an array element with a sequence.\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_12514]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_train_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_validation_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\modeling_tf_utils.py:1209\u001b[0m, in \u001b[0;36mTFPreTrainedModel.fit\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(keras\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mfit)\n\u001b[0;32m   1207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1208\u001b[0m     args, kwargs \u001b[38;5;241m=\u001b[39m convert_batch_encoding(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m-> 1209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mD:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node EagerPyFunc defined at (most recent call last):\n<stack traces unavailable>\nError in user-defined function passed to MapDataset:16 transformation with iterator: Iterator::Root::Prefetch::ParallelMapV2::ParallelMapV2::Map: ValueError: setting an array element with a sequence.\nTraceback (most recent call last):\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 267, in __call__\n    return func(device, token, args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 145, in __call__\n    outputs = self._call(device, args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\ops\\script_ops.py\", line 152, in _call\n    ret = self._func(*args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\datasets\\utils\\tf_utils.py\", line 99, in np_get_batch\n    batch = collate_fn(batch, **collate_fn_args)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 44, in __call__\n    return self.tf_call(features)\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 395, in tf_call\n    batch = {k: tf.convert_to_tensor(v, dtype=tf.int64) for k, v in batch.items()}\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\transformers\\data\\data_collator.py\", line 395, in <dictcomp>\n    batch = {k: tf.convert_to_tensor(v, dtype=tf.int64) for k, v in batch.items()}\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\", line 153, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n\n  File \"D:\\otake\\gpu-service\\users\\vidzshan\\venv\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 108, in convert_to_eager_tensor\n    return ops.EagerTensor(value, ctx.device_name, dtype)\n\nValueError: setting an array element with a sequence.\n\n\n\t [[{{node EagerPyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_12514]"
     ]
    }
   ],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0a040935-fac7-45a3-bc70-237600ea5753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Root Cause:\n",
      "When using model.fit(...) with a tf.data.Dataset (created by model.prepare_tf_dataset(...)), the collate function (i.e., data_collator) is trying to convert a batch of variable-length sequences into a fixed-shape tensor.\n",
      "This fails if:\n",
      "\n",
      "The features (like labels, input_ids, etc.) have sequences of different lengths.\n",
      "\n",
      "The batch contains Python lists of lists (ragged) that TensorFlow can't convert directly to tf.Tensor.\n"
     ]
    }
   ],
   "source": [
    "print(''' Root Cause:\n",
    "When using model.fit(...) with a tf.data.Dataset (created by model.prepare_tf_dataset(...)), the collate function (i.e., data_collator) is trying to convert a batch of variable-length sequences into a fixed-shape tensor.\n",
    "This fails if:\n",
    "\n",
    "The features (like labels, input_ids, etc.) have sequences of different lengths.\n",
    "\n",
    "The batch contains Python lists of lists (ragged) that TensorFlow can't convert directly to tf.Tensor.''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2c18b1-6ca7-47dc-9998-e409362dbd57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03e3fd9cf18f490981a2b33537e3afc0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "TextStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "079e7349b354470d98153da82825e9fe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "13f8c31fb1394616b1cc61d8bf4a9150": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "PasswordModel",
      "state": {
       "description": "Token:",
       "layout": "IPY_MODEL_c064cc84839a42219eb976d828996d59",
       "style": "IPY_MODEL_03e3fd9cf18f490981a2b33537e3afc0"
      }
     },
     "1eff5d93531a41119b5c87c2cc5a940d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c0824c95348a45a5bc1e79e2789a4101",
       "style": "IPY_MODEL_453bf712ec314f909ab34c425c9bffa8",
       "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
      }
     },
     "453bf712ec314f909ab34c425c9bffa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "56913448fe7b438f88fe3b9dd7996022": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6ad55d5c76724ea7ae50437a6a54b493": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonModel",
      "state": {
       "description": "Login",
       "layout": "IPY_MODEL_079e7349b354470d98153da82825e9fe",
       "style": "IPY_MODEL_a6963252d63d4556b577744721727dd8",
       "tooltip": null
      }
     },
     "97d2e1b19e4d42038173394becefa896": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "VBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_1eff5d93531a41119b5c87c2cc5a940d",
        "IPY_MODEL_13f8c31fb1394616b1cc61d8bf4a9150",
        "IPY_MODEL_fed4a723ea6541078099e335cd5c9a31",
        "IPY_MODEL_6ad55d5c76724ea7ae50437a6a54b493",
        "IPY_MODEL_c5940160af2c4f74bb12b00fcb2e9bda"
       ],
       "layout": "IPY_MODEL_b8d5db9fb2894e2bbfcc22cca4247751"
      }
     },
     "9e1deb6409954eb9a176d898ae742fc4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a6963252d63d4556b577744721727dd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ButtonStyleModel",
      "state": {
       "font_family": null,
       "font_size": null,
       "font_style": null,
       "font_variant": null,
       "font_weight": null,
       "text_color": null,
       "text_decoration": null
      }
     },
     "b8d5db9fb2894e2bbfcc22cca4247751": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "align_items": "center",
       "display": "flex",
       "flex_flow": "column",
       "width": "50%"
      }
     },
     "bc7b8e9344e14b84bedebdf5d68b9348": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "c064cc84839a42219eb976d828996d59": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c0824c95348a45a5bc1e79e2789a4101": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c5940160af2c4f74bb12b00fcb2e9bda": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_9e1deb6409954eb9a176d898ae742fc4",
       "style": "IPY_MODEL_fbb07b030b084fc6b185d7cf79c9d1d1",
       "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
      }
     },
     "fbb07b030b084fc6b185d7cf79c9d1d1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "fed4a723ea6541078099e335cd5c9a31": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "CheckboxModel",
      "state": {
       "description": "Add token as git credential?",
       "disabled": false,
       "layout": "IPY_MODEL_56913448fe7b438f88fe3b9dd7996022",
       "style": "IPY_MODEL_bc7b8e9344e14b84bedebdf5d68b9348",
       "value": true
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
