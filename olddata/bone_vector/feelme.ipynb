{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ce8cdc9-66b1-43b1-a1e8-99b1b35d8fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained weights found at vitg-384.pt and loaded with msg: <All keys matched successfully>\n",
      "\n",
      "HuggingFace output shape: torch.Size([1, 18432, 1408])\n",
      "PyTorch output shape: torch.Size([1, 18432, 1408])\n",
      "Absolute difference sum: 11951387.000000\n",
      "Close: False\n",
      "\n",
      "Pretrained weights found and loaded with msg: <All keys matched successfully>\n",
      "Classifier output shape: torch.Size([1, 174])\n",
      "Top 5 predicted class names:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ymtla\\AppData\\Local\\Temp\\ipykernel_24016\\2429805479.py:85: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  top5_probs = F.softmax(out_classifier.topk(100).values[0]) * 100.0  # convert to percentage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing [something] to the camera (62.27323532104492%)\n",
      "Spinning [something] so it continues spinning (15.901102066040039%)\n",
      "Rolling [something] on a flat surface (2.63826060295105%)\n",
      "Throwing [something] (2.478363513946533%)\n",
      "Pushing [something] so it spins (1.733525276184082%)\n",
      "Letting [something] roll along a flat surface (1.6069393157958984%)\n",
      "Poking [something] so that it spins around (1.201206922531128%)\n",
      "Twisting [something] (0.9632881879806519%)\n",
      "Turning the camera left while filming [something] (0.863765299320221%)\n",
      "Spreading [something] onto [something] (0.6216221451759338%)\n",
      "Showing a photo of [something] to the camera (0.5682592391967773%)\n",
      "Moving [part] of [something] (0.5400030612945557%)\n",
      "Pretending to spread air onto [something] (0.40627753734588623%)\n",
      "Moving [something] across a surface without it falling down (0.39038965106010437%)\n",
      "Pretending to turn [something] upside down (0.3809271454811096%)\n",
      "Moving [something] and [something] so they pass each other (0.35168275237083435%)\n",
      "Moving [something] up (0.3367186188697815%)\n",
      "Lifting [something] up completely without letting it drop down (0.3198796510696411%)\n",
      "Pushing [something] from right to left (0.31929701566696167%)\n",
      "Turning [something] upside down (0.29372045397758484%)\n",
      "Stacking [number of] [something] (0.28610068559646606%)\n",
      "Spinning [something] that quickly stops spinning (0.2850147783756256%)\n",
      "Pretending to take [something] from [somewhere] (0.28287971019744873%)\n",
      "Moving [something] away from the camera (0.23701417446136475%)\n",
      "Moving [something] towards the camera (0.23237648606300354%)\n",
      "Putting [something similar to other things that are already on the table] (0.22437900304794312%)\n",
      "Pushing [something] from left to right (0.20948347449302673%)\n",
      "Throwing [something] onto a surface (0.20362669229507446%)\n",
      "Taking [something] from [somewhere] (0.18669100105762482%)\n",
      "Throwing [something] in the air and letting it fall (0.18557606637477875%)\n",
      "Showing [something] on top of [something] (0.17242269217967987%)\n",
      "Pretending or trying and failing to twist [something] (0.1655690222978592%)\n",
      "Putting [something] onto [something] (0.16235719621181488%)\n",
      "[Something] falling like a feather or paper (0.15064677596092224%)\n",
      "Showing that [something] is empty (0.1472640037536621%)\n",
      "Approaching [something] with your camera (0.13476645946502686%)\n",
      "Pushing [something] so that it slightly moves (0.11017904430627823%)\n",
      "Picking [something] up (0.10974067449569702%)\n",
      "Putting [something] on a surface (0.1077340766787529%)\n",
      "Holding [something] (0.10607989877462387%)\n",
      "Holding [something] in front of [something] (0.09932971745729446%)\n",
      "Bending [something] so that it deforms (0.0948977991938591%)\n",
      "Pretending to poke [something] (0.08899205923080444%)\n",
      "Pretending to pick [something] up (0.08538314700126648%)\n",
      "Turning the camera right while filming [something] (0.08344454318284988%)\n",
      "Pretending to scoop [something] up with [something] (0.07014395296573639%)\n",
      "Pretending to sprinkle air onto [something] (0.06403982639312744%)\n",
      "Touching (without moving) [part] of [something] (0.05930374562740326%)\n",
      "Pulling [something] from left to right (0.05901933088898659%)\n",
      "Pouring [something] into [something] (0.055855121463537216%)\n",
      "Sprinkling [something] onto [something] (0.05496840551495552%)\n",
      "Unfolding [something] (0.054847076535224915%)\n",
      "Lifting [something] with [something] on it (0.05086348205804825%)\n",
      "Holding [something] over [something] (0.0506884828209877%)\n",
      "Moving [something] away from [something] (0.04941227287054062%)\n",
      "Putting [something] on a flat surface without letting it roll (0.04837005212903023%)\n",
      "Piling [something] up (0.04786354675889015%)\n",
      "Pouring [something] out of [something] (0.04773504287004471%)\n",
      "Wiping [something] off of [something] (0.04699764773249626%)\n",
      "Showing [something] next to [something] (0.04125415161252022%)\n",
      "Showing [something] behind [something] (0.04037277400493622%)\n",
      "[Something] being deflected from [something] (0.037789590656757355%)\n",
      "Letting [something] roll down a slanted surface (0.0360642746090889%)\n",
      "Putting [something] that can't roll onto a slanted surface, so it slides down (0.03604615479707718%)\n",
      "[Something] falling like a rock (0.03514084964990616%)\n",
      "Lifting [something] up completely, then letting it drop down (0.03272465988993645%)\n",
      "Pouring [something] onto [something] (0.03211592882871628%)\n",
      "Tearing [something] into two pieces (0.03165585175156593%)\n",
      "Moving away from [something] with your camera (0.028902394697070122%)\n",
      "Scooping [something] up with [something] (0.02875223569571972%)\n",
      "Pretending to open [something] without actually opening it (0.028605090454220772%)\n",
      "Taking [one of many similar things on the table] (0.02689274773001671%)\n",
      "Pretending to pour [something] out of [something], but [something] is empty (0.02634209208190441%)\n",
      "Poking [something] so lightly that it doesn't or almost doesn't move (0.025817256420850754%)\n",
      "Showing that [something] is inside [something] (0.023662526160478592%)\n",
      "Folding [something] (0.022544987499713898%)\n",
      "Dropping [something] onto [something] (0.021906988695263863%)\n",
      "Pretending to throw [something] (0.02164088562130928%)\n",
      "Moving [something] and [something] away from each other (0.021505242213606834%)\n",
      "Pretending to take [something] out of [something] (0.020566586405038834%)\n",
      "Throwing [something] in the air and catching it (0.019813409075140953%)\n",
      "Putting [something] behind [something] (0.019184064120054245%)\n",
      "Pretending to squeeze [something] (0.018089139834046364%)\n",
      "Trying to bend [something unbendable] so nothing happens (0.01698484644293785%)\n",
      "Twisting (wringing) [something] wet until water comes out (0.016458572819828987%)\n",
      "Opening [something] (0.016224773600697517%)\n",
      "Pretending or failing to wipe [something] off of [something] (0.016193008050322533%)\n",
      "Holding [something] next to [something] (0.015975456684827805%)\n",
      "Pretending to be tearing [something that is not tearable] (0.01538811158388853%)\n",
      "Lifting up one end of [something] without letting it drop down (0.015355318784713745%)\n",
      "Putting [something] that can't roll onto a slanted surface, so it stays where it is (0.014089448377490044%)\n",
      "Pretending to put [something] on a surface (0.013971420004963875%)\n",
      "Putting [something] next to [something] (0.013746106065809727%)\n",
      "Pulling [something] from right to left (0.011586226522922516%)\n",
      "Moving [something] across a surface until it falls down (0.0109456991776824%)\n",
      "[Something] colliding with [something] and both are being deflected (0.009219479747116566%)\n",
      "Letting [something] roll up a slanted surface, so it rolls back down (0.009171527810394764%)\n",
      "Turning the camera upwards while filming [something] (0.008983967825770378%)\n",
      "Poking a stack of [something] without the stack collapsing (0.008945921435952187%)\n",
      "Pushing [something] so that it falls off the table (0.008849907666444778%)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from decord import VideoReader\n",
    "from transformers import AutoVideoProcessor, AutoModel\n",
    "\n",
    "import src.datasets.utils.video.transforms as video_transforms\n",
    "import src.datasets.utils.video.volume_transforms as volume_transforms\n",
    "from src.models.attentive_pooler import AttentiveClassifier\n",
    "from src.models.vision_transformer import vit_giant_xformers_rope\n",
    "\n",
    "IMAGENET_DEFAULT_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_DEFAULT_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "def load_pretrained_vjepa_pt_weights(model, pretrained_weights):\n",
    "    # Load weights of the VJEPA2 encoder\n",
    "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
    "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"encoder\"]\n",
    "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    pretrained_dict = {k.replace(\"backbone.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
    "\n",
    "\n",
    "def load_pretrained_vjepa_classifier_weights(model, pretrained_weights):\n",
    "    # Load weights of the VJEPA2 classifier\n",
    "    # The PyTorch state_dict is already preprocessed to have the right key names\n",
    "    pretrained_dict = torch.load(pretrained_weights, weights_only=True, map_location=\"cpu\")[\"classifiers\"][0]\n",
    "    pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "    msg = model.load_state_dict(pretrained_dict, strict=False)\n",
    "    print(\"Pretrained weights found at {} and loaded with msg: {}\".format(pretrained_weights, msg))\n",
    "\n",
    "\n",
    "def build_pt_video_transform(img_size):\n",
    "    short_side_size = int(256.0 / 224 * img_size)\n",
    "    # Eval transform has no random cropping nor flip\n",
    "    eval_transform = video_transforms.Compose(\n",
    "        [\n",
    "            video_transforms.Resize(short_side_size, interpolation=\"bilinear\"),\n",
    "            video_transforms.CenterCrop(size=(img_size, img_size)),\n",
    "            volume_transforms.ClipToTensor(),\n",
    "            video_transforms.Normalize(mean=IMAGENET_DEFAULT_MEAN, std=IMAGENET_DEFAULT_STD),\n",
    "        ]\n",
    "    )\n",
    "    return eval_transform\n",
    "\n",
    "\n",
    "def get_video():\n",
    "    vr = VideoReader(\"sample/vjepa2_test1_1.mp4\")\n",
    "    # choosing some frames here, you can define more complex sampling strategy\n",
    "    frame_idx = np.arange(0, 128, 2)\n",
    "    video = vr.get_batch(frame_idx).asnumpy()\n",
    "    return video\n",
    "\n",
    "\n",
    "def forward_vjepa_video(model_hf, model_pt, hf_transform, pt_transform):\n",
    "    # Run a sample inference with VJEPA\n",
    "    with torch.inference_mode():\n",
    "        # Read and pre-process the image\n",
    "        video = get_video()  # T x H x W x C\n",
    "        video = torch.from_numpy(video).permute(0, 3, 1, 2)  # T x C x H x W\n",
    "        x_pt = pt_transform(video).cuda().unsqueeze(0)\n",
    "        x_hf = hf_transform(video, return_tensors=\"pt\")[\"pixel_values_videos\"].to(\"cuda\")\n",
    "        # Extract the patch-wise features from the last layer\n",
    "        out_patch_features_pt = model_pt(x_pt)\n",
    "        out_patch_features_hf = model_hf.get_vision_features(x_hf)\n",
    "\n",
    "    return out_patch_features_hf, out_patch_features_pt\n",
    "\n",
    "\n",
    "def get_vjepa_video_classification_results(classifier, out_patch_features_pt):\n",
    "    SOMETHING_SOMETHING_V2_CLASSES = json.load(open(\"ssv2_classes.json\", \"r\"))\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        out_classifier = classifier(out_patch_features_pt)\n",
    "\n",
    "    print(f\"Classifier output shape: {out_classifier.shape}\")\n",
    "\n",
    "    print(\"Top 5 predicted class names:\")\n",
    "    top5_indices = out_classifier.topk(100).indices[0]\n",
    "    top5_probs = F.softmax(out_classifier.topk(100).values[0]) * 100.0  # convert to percentage\n",
    "    for idx, prob in zip(top5_indices, top5_probs):\n",
    "        str_idx = str(idx.item())\n",
    "        print(f\"{SOMETHING_SOMETHING_V2_CLASSES[str_idx]} ({prob}%)\")\n",
    "\n",
    "    return\n",
    "\n",
    "# HuggingFace model repo name\n",
    "hf_model_name = (\n",
    "    \"facebook/vjepa2-vitg-fpc64-384\"  # Replace with your favored model, e.g. facebook/vjepa2-vitg-fpc64-384\n",
    ")\n",
    "# Path to local PyTorch weights\n",
    "#pt_model_path = \"YOUR_MODEL_PATH\"\n",
    "\n",
    "# Initialize the HuggingFace model, load pretrained weights\n",
    "model_hf = AutoModel.from_pretrained(hf_model_name)\n",
    "model_hf.cuda().eval()\n",
    "\n",
    "# Build HuggingFace preprocessing transform\n",
    "hf_transform = AutoVideoProcessor.from_pretrained(hf_model_name)\n",
    "img_size = hf_transform.crop_size[\"height\"]  # E.g. 384, 256, etc.\n",
    "\n",
    "# Initialize both models\n",
    "model_pt = vit_giant_xformers_rope(img_size=(img_size, img_size), num_frames=64)\n",
    "model_pt.cuda().eval()\n",
    "pt_model_path = \"vitg-384.pt\"\n",
    "load_pretrained_vjepa_pt_weights(model_pt, pt_model_path)\n",
    "\n",
    "pt_video_transform = build_pt_video_transform(img_size=img_size)\n",
    "out_patch_features_hf, out_patch_features_pt = forward_vjepa_video(model_hf, model_pt, hf_transform, pt_video_transform)\n",
    "\n",
    "print(f\"\"\"\n",
    "HuggingFace output shape: {out_patch_features_hf.shape}\n",
    "PyTorch output shape: {out_patch_features_pt.shape}\n",
    "Absolute difference sum: {torch.abs(out_patch_features_pt - out_patch_features_hf).sum():.6f}\n",
    "Close: {torch.allclose(out_patch_features_pt, out_patch_features_hf, atol=1e-3, rtol=1e-3)}\n",
    "\"\"\")\n",
    "\n",
    "# Load classifier\n",
    "classifier_model_path = \"ssv2-vitg-384-64x2x3.pt\"\n",
    "assert os.path.exists(classifier_model_path), \"Classifier model not found!\"\n",
    "\n",
    "classifier = AttentiveClassifier(embed_dim=model_hf.config.hidden_size, num_heads=16, depth=4, num_classes=174).cuda().eval()\n",
    "pretrained_dict = torch.load(classifier_model_path, weights_only=True, map_location=\"cpu\")[\"classifiers\"][0]\n",
    "pretrained_dict = {k.replace(\"module.\", \"\"): v for k, v in pretrained_dict.items()}\n",
    "msg = classifier.load_state_dict(pretrained_dict, strict=False)\n",
    "print(\"Pretrained weights found and loaded with msg: {}\".format(msg))\n",
    "\n",
    "get_vjepa_video_classification_results(classifier, out_patch_features_pt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cacb0a42-9efe-4899-9e89-6102882a9b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ドライブ D のボリューム ラベルは ボリューム です\n",
      " ボリューム シリアル番号は ECD2-8FC7 です\n",
      "\n",
      " D:\\otake\\gpu-service\\users\\vidzshan\\notebooks\\Finalvjepa2\\vjepa2 のディレクトリ\n",
      "\n",
      "2025/07/03  11:07    <DIR>          .\n",
      "2025/07/03  10:09    <DIR>          ..\n",
      "2025/07/03  10:09               120 .flake8\n",
      "2025/07/03  10:09    <DIR>          .github\n",
      "2025/07/03  10:09               499 .gitignore\n",
      "2025/07/03  10:39    <DIR>          .ipynb_checkpoints\n",
      "2025/07/03  10:09            11,349 APACHE-LICENSE\n",
      "2025/07/03  10:09    <DIR>          app\n",
      "2025/07/03  10:09    <DIR>          assets\n",
      "2025/07/03  10:09                74 CHANGELOG.md\n",
      "2025/07/03  10:09             3,535 CODE_OF_CONDUCT.md\n",
      "2025/07/03  10:09    <DIR>          configs\n",
      "2025/07/03  10:09             1,504 CONTRIBUTING.md\n",
      "2025/07/03  10:09    <DIR>          evals\n",
      "2025/07/03  10:09               429 hubconf.py\n",
      "2025/07/03  10:09             1,087 LICENSE\n",
      "2025/07/03  10:09    <DIR>          notebooks\n",
      "2025/07/03  10:09                77 pyproject.toml\n",
      "2025/07/03  10:09            17,245 README.md\n",
      "2025/07/03  10:09               212 requirements.txt\n",
      "2025/07/03  10:09                76 requirements-test.txt\n",
      "2025/07/03  10:11           443,780 sample_video.mp4\n",
      "2025/07/03  10:09               689 setup.py\n",
      "2025/07/03  10:10    <DIR>          src\n",
      "2025/07/03  11:03       373,996,872 ssv2-vitg-384-64x2x3.pt\n",
      "2025/07/03  10:09    <DIR>          tests\n",
      "2025/07/03  11:07            15,410 Untitled.ipynb\n",
      "              16 個のファイル         374,492,958 バイト\n",
      "              11 個のディレクトリ  3,683,154,685,952 バイトの空き領域\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c5981a-b890-438f-aa7f-8f6f484247b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b31c9c-c06c-4310-86e1-06672d0e274c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
